{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model\n",
    "\n",
    "In this script, the code for preprocessing, processing, and all modeling is performed.\n",
    "\n",
    "First the read from csv files, filtered and brought to a format readable by the ML function.\n",
    "\n",
    "3 datasets are created, for 100, 500, and 1000 meter SAR resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from numpy import mean, std\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    silhouette_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    max_error,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_validate, KFold, StratifiedKFold, GridSearchCV, LearningCurveDisplay\n",
    "from sklearn.preprocessing import normalize, StandardScaler, LabelEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "\n",
    "workspace_directory = r'C:\\Users\\trygg\\Documents\\Master_project'  # HOME\n",
    "# workspace_directory = r\"C:\\Users\\tryggvisi\\Documents\\my-awesome-masters-project\"  # WORK\n",
    "# Change the current working directory to the specified workspace\n",
    "os.chdir(workspace_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_the_data(file_old, file_era5, resolution, temperature, std):\n",
    "    # Step 1: Read the main CSV datafile, either old and ERA5 data or just the old data\n",
    "    path_to_folder = r\"Data\\Vindefjallen_data\\Vindefjallen_cleaning\\exploratory_data_analysis\\2_values_from_raster_python_R\"\n",
    "    if file_era5 != 'none':\n",
    "        path_old = os.path.join(path_to_folder, file_old)\n",
    "        df_data1 = pd.read_csv(path_old)\n",
    "        df_data1[\"original\"] = 0  # 0 for old data\n",
    "\n",
    "        path_era5 = os.path.join(path_to_folder, file_era5)\n",
    "        df_data2 = pd.read_csv(path_era5)\n",
    "        df_data2[\"original\"] = 1  # 1 for ERA5 data\n",
    "        df_data = pd.concat([df_data1, df_data2], ignore_index=True)\n",
    "    elif file_era5 == 'none':\n",
    "        path_old = os.path.join(path_to_folder, file_old)\n",
    "        df_data = pd.read_csv(path_old)\n",
    "        df_data[\"original\"] = 0\n",
    "\n",
    "    # Step 2: Rename columns in the main data frame\n",
    "    df_data.rename(columns={\"ID\": \"ID_clean\", \"OG_ID\": \"ID_preclean\", \"MiddleTime\": \"middleTime\", \"SnowDepth\": \"snow_depth\",\n",
    "                            \"point_VH\": \"VH\", \"point_VV\": \"VV\", \"points_angle\": \"angle\"}, inplace=True)\n",
    "\n",
    "    # Step 3: Read the DEM dataset and prepare it for merging\n",
    "    path_file = r\"Data\\Vindefjallen_data\\Vindefjallen_cleaning\\exploratory_data_analysis\\4_DEM_data\\DEM_extracted_values.csv\"\n",
    "    df_DEM = pd.read_csv(path_file)\n",
    "    df_DEM = df_DEM.drop([\"FID\", \"OG_ID\", \"Triangle\", \"Corner\", \"TriangleCo\", \"date\", \"SnowDepth\", \"SnowDepth_\",\n",
    "                          \"temperatur\", \"x_4326\", \"y_4326\", \"x_3006\", \"y_3006\", \"MiddleTime\"], axis=1)\n",
    "\n",
    "    # Step 4: Read the ERA5 dataset\n",
    "    # with 3 parameters and complete list.THIS IS OLD AND BAR ERA5\n",
    "    path_data_era = r\"Scripts\\Python\\ERA5\\era5_temp\\era5_vindefjallen_data_2023_3params_new.csv\"\n",
    "    df_read_era = pd.read_csv(path_data_era)\n",
    "\n",
    "    # read the era5-land dataset\n",
    "    # path_data_era_land = r\"Scripts\\Python\\ERA5\\era5_temp\\era5_land_vindefjallen_data_2023_4params.csv\" #með skt,sde,sd,rsn\n",
    "    # með skt,sde,sd + vgt h og l og snow temp\n",
    "    path_data_era_land = r\"Scripts\\Python\\ERA5\\era5_temp\\era5_land_vindefjallen_data_2023_7params.csv\"\n",
    "    df_read_era_land = pd.read_csv(path_data_era_land)\n",
    "\n",
    "    # Step 5b: Read vegetation type dataset and prepare it for merging\n",
    "    read_vegtyp = r\"Data\\Landcover\\Derived_datasets\\points_with_vegtyp.xls\"\n",
    "    df_vegtyp = pd.read_excel(read_vegtyp)\n",
    "    df_vegtyp = df_vegtyp.drop_duplicates(subset=['TriangleCo'])\n",
    "    df_vegtyp.reset_index(drop=True, inplace=True)\n",
    "    df_vegtyp = df_vegtyp[['TriangleCo', 'VEGETATION', 'VEGTYP']]\n",
    "\n",
    "    # Step 6: Merge with DEM dataset\n",
    "    df_merge_DEM = pd.merge(left=df_data, right=df_DEM, left_on=[\n",
    "                            \"ID_clean\"], right_on=[\"ID\"], how=\"left\")\n",
    "\n",
    "    # Step 7: Merge with ERA5 dataset\n",
    "    df_merge_ERA5 = pd.merge(left=df_merge_DEM, right=df_read_era, left_on=[\n",
    "                             \"ID_clean\"], right_on=[\"ID\"], how=\"left\")\n",
    "\n",
    "    df = pd.merge(left=df_merge_ERA5, right=df_read_era_land, left_on=[\n",
    "                  \"ID_clean\"], right_on=[\"ID\"], how=\"left\")\n",
    "\n",
    "    # merge with df_vegtyp\n",
    "    df = pd.merge(left=df, right=df_vegtyp, left_on=[\n",
    "        \"TriangleCorner\"], right_on=[\"TriangleCo\"], how=\"left\")\n",
    "\n",
    "    # Step 9: Convert snow depth to integer and remove rows with '+'\n",
    "    df = df[~df[\"snow_depth\"].str.contains(r\"\\d+\\+\")]\n",
    "    df[\"snow_depth\"] = df[\"snow_depth\"].astype(np.int64)\n",
    "\n",
    "    # Step 10: Remove rows with NaN in VH column and one outlier row\n",
    "    df = df.dropna(subset=[\"VH\"])\n",
    "\n",
    "    # step 10.1: AC-28C because it's on a lake.\n",
    "    df = df[df[\"TriangleCorner\"] != \"AC-28C\"]\n",
    "\n",
    "    # Step 11: Remove rows with geometric distortions\n",
    "    df = df[df[\"points_mask\"] == 0]\n",
    "\n",
    "    # Step 13: Calculate VH/VV ratio\n",
    "    df[\"VH/VV\"] = df[\"VH\"] / df[\"VV\"]\n",
    "\n",
    "    # Step 14: Filter data based on standard deviations\n",
    "    # df = df[np.abs(df[\"VH/VV\"] - df[\"VH/VV\"].mean()) <= (std * df[\"VH/VV\"].std())]\n",
    "    df = df[np.abs(df[\"VH\"] - df[\"VH\"].mean()) <= (std * df[\"VH\"].std())]\n",
    "    df = df[np.abs(df[\"VV\"] - df[\"VV\"].mean()) <= (std * df[\"VV\"].std())]\n",
    "\n",
    "    # Step 15: One-hot encode orbitProp\n",
    "    df[[\"orbitProp_ASCENDING\", \"orbitProp_DESCENDING\"]\n",
    "       ] = df[\"orbitProp\"].str.get_dummies()\n",
    "\n",
    "    # Step 16: Impute missing values in the temperature column\n",
    "    if temperature == \"drop\":\n",
    "        df = df.dropna(subset=[\"temperature\"])\n",
    "        print(\"Dropping rows with missing values in temperature\")\n",
    "    elif temperature in [\"most_frequent\", \"mean\"]:\n",
    "        imp = SimpleImputer(strategy=temperature)\n",
    "        df[\"temperature\"] = imp.fit_transform(df[[\"temperature\"]])\n",
    "        print(\n",
    "            f\"Imputing missing values in temperature with {temperature} value\")\n",
    "    elif temperature == \"skip\":\n",
    "        print(\"Skipping temperature imputation\")\n",
    "    else:\n",
    "        print(\"No valid input for temperature imputation\")\n",
    "\n",
    "    # Step 17: Select columns to keep\n",
    "    df = df[[\"ID_clean\", \"TriangleCorner\", \"date\", \"snow_depth\", \"time_difference\",\n",
    "            #  \"temperature\", \"middleTime\", \"orbitProp_DESCENDING\",\"orbitProp\",\n",
    "             \"VH\", \"VV\", \"VH/VV\", \"angle\", \"TPI_100\",  \"TRI_100\", \"ASP_100\", \"SLOP_100\", \"ELEV_100\",\n",
    "             \"TRI_500\",  \"ELEV_500\",  \"ASP_500\", \"SLO_500\", \"ASP_1000\", \"SLO_1000\", \"TRI_1000\", \"ELEV_1000\", \"TPI_500\", \"TPI_1000\",\n",
    "             'VEGETATION', 'VEGTYP',\n",
    "             \"orbitProp_ASCENDING\",\n",
    "             \"original\",\n",
    "             \"t2m\", \"rsn\", \"sd\",\n",
    "             \"era5_l_rsn\",  \"era5_l_sd\", \"era5_l_skt\", \"era5_l_sde\",\n",
    "             \"era5_l_lai_hv\", \"era5_l_lai_lv\", \"era5_l_tsn\",\n",
    "             ]]\n",
    "\n",
    "    # Step 18: Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Step 19: Convert time_difference to integer hours and round to nearest hour\n",
    "    for i in range(len(df)):\n",
    "        time_components = df.loc[i, \"time_difference\"].split(\":\")\n",
    "        if len(time_components) == 3:\n",
    "            hours, minutes, seconds = map(int, time_components)\n",
    "        elif len(time_components) == 2:\n",
    "            hours, minutes = map(int, time_components)\n",
    "            seconds = 0\n",
    "        else:\n",
    "            raise ValueError(\"Invalid time string format\")\n",
    "        total_hours = round(hours + minutes / 60 + seconds / 3600)\n",
    "        df.loc[i, \"time_difference_rnd\"] = total_hours\n",
    "\n",
    "    df[\"snow_depth\"] = df[\"snow_depth\"].apply(lambda x: round(x, -1))\n",
    "\n",
    "    df = df[np.abs(df[\"TRI_100\"] - df[\"TRI_100\"].mean())\n",
    "            <= (4 * df[\"TRI_100\"].std())]\n",
    "    # converting snow depth to real snow depth\n",
    "\n",
    "    df['resolution'] = resolution\n",
    "    df['era5_l_sde'] = df['era5_l_sde'] * 100\n",
    "\n",
    "    # filter snow depths over 50 and less than 300\n",
    "    # df = df[(df['snow_depth'] > 90) & (df['snow_depth'] < 200)]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping temperature imputation\n",
      "(475, 40)\n",
      "Skipping temperature imputation\n",
      "(477, 40)\n",
      "Skipping temperature imputation\n",
      "(485, 40)\n"
     ]
    }
   ],
   "source": [
    "data_100 = r\"values_to_points_R_100.csv\"\n",
    "data_500 = r\"values_to_points_R_500.csv\"\n",
    "data_1000 = r\"values_to_points_R_1000.csv\"\n",
    "data_era5_100 = r\"values_to_points_R_era5_100.csv\"\n",
    "data_era5_500 = r\"values_to_points_R_era5_500.csv\"\n",
    "data_era5_1000 = r\"values_to_points_R_era5_1000.csv\"\n",
    "\n",
    "temperature = \"skip\" # \"drop\", \"mean\", \"most_frequent\", \"skip\". This is for temperature measurements from the in situ data. It was not used in the final model. \n",
    "std = 3 # Standard deviation for filtering out outliers.\n",
    "df_100 = cleaning_the_data(data_100, data_era5_100,\n",
    "                           '100', temperature=temperature, std=std)\n",
    "print(df_100.shape)\n",
    "df_500 = cleaning_the_data(data_500, data_era5_500,\n",
    "                           '500', temperature=temperature, std=std)\n",
    "print(df_500.shape)\n",
    "df_1000 = cleaning_the_data(\n",
    "    data_1000, data_era5_1000, '1000', temperature=temperature, std=std)\n",
    "print(df_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XBoost experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Dataset preparation function\n",
    "\n",
    "\n",
    "def dataset_prep(df, model_group):\n",
    "    features_SAR = [\n",
    "        'era5_l_sde',\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "    ]\n",
    "\n",
    "    features_SAR_terr = [\n",
    "        'era5_l_sde',\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "        'TRI_1000', 'ELEV_1000', 'ASP_1000', 'SLO_1000', 'TPI_1000',\n",
    "    ]\n",
    "\n",
    "    features_sar_terr_era = [\n",
    "        'era5_l_sde',\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "        'TRI_1000', 'ELEV_1000', 'ASP_1000', 'SLO_1000', 'TPI_1000',\n",
    "        \"era5_l_skt\",\n",
    "        \"era5_l_rsn\",\n",
    "        \"era5_l_sd\",\n",
    "    ]\n",
    "\n",
    "    if model_group == 'SAR':\n",
    "        X = df[features_SAR]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "\n",
    "    elif model_group == 'terrain':\n",
    "        X = df[features_SAR_terr]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "\n",
    "    elif model_group == 'era5_land':\n",
    "        X = df[features_sar_terr_era]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Example of loading your data\n",
    "df = df_1000\n",
    "\n",
    "# Prepare the dataset\n",
    "# You can change 'SAR' to 'terrain' or 'era5_land' as needed\n",
    "X, y = dataset_prep(df, 'era5_land')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test, X_train_rsd, X_test_rsd = split_data(\n",
    "    X, y, random_state=42)\n",
    "\n",
    "# Define the Gradient Boosting model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Hyperparameters for tuning\n",
    "params = {'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "          'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "          'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "          'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "          'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "          'n_estimators': [100, 300, 500, 800, 1000]}\n",
    "\n",
    "v_outer = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Grid search with cross-validation\n",
    "grid_search = RandomizedSearchCV(estimator=model, param_distributions=params, cv=v_outer,\n",
    "                                 scoring='neg_mean_squared_error', n_iter=60, verbose=1, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Lowest RMSE: \", (-grid_search.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Example usage:\n",
    "# Uncomment the following lines and replace 'your_dataset.csv' with your actual dataset path.\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# X, y = dataset_prep(df, 'SAR')  # Choose 'SAR', 'terrain', or 'era5_land'\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "# print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset prep. Preparing the dataset for modeling and creating X and y variables.\n",
    "# era5_l_sde is not used for modeling. This is a workaround so it can be used later to compare with the predictions and it matches\n",
    "# with the index of the input data.\n",
    "def dataset_prep(df, model_group):\n",
    "    features_SAR = [\n",
    "        'era5_l_sde',\n",
    "\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "    ]\n",
    "    features_SAR_terr = [\n",
    "        'era5_l_sde',\n",
    "\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "        # \"ELEV_100\",\"ASP_100\",\"SLOP_100\",\"TPI_100\",\"TRI_100\",\n",
    "        # 'TRI_500', 'ELEV_500', 'ASP_500', 'SLO_500', 'TPI_500',\n",
    "        'TRI_1000', 'ELEV_1000', 'ASP_1000', 'SLO_1000', 'TPI_1000',\n",
    "    ]\n",
    "    features_sar_terr_era = [\n",
    "        'era5_l_sde',\n",
    "        \"VH\",\n",
    "        \"VV\",\n",
    "        \"VH/VV\",\n",
    "        \"angle\",\n",
    "        \"time_difference_rnd\",\n",
    "        'orbitProp_ASCENDING',\n",
    "\n",
    "        # \"ELEV_100\",\"ASP_100\",\"SLOP_100\", \"TPI_100\",\"TRI_100\",\n",
    "        # 'TRI_500', 'ELEV_500', 'ASP_500', 'SLO_500', 'TPI_500',\n",
    "        'TRI_1000', 'ELEV_1000', 'ASP_1000', 'SLO_1000', 'TPI_1000',\n",
    "        \"era5_l_skt\",\n",
    "        \"era5_l_rsn\",\n",
    "        \"era5_l_sd\",\n",
    "    ]\n",
    "    if model_group == 'SAR':\n",
    "        X = df[features_SAR]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "\n",
    "    if model_group == 'terrain':\n",
    "        X = df[features_SAR_terr]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "\n",
    "    if model_group == 'era5_land':\n",
    "        X = df[features_sar_terr_era]\n",
    "        y = df['snow_depth']\n",
    "        print('df prep ', X.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 75% training and 25% testing\n",
    "def split_data(X, y, random_state):\n",
    "\n",
    "    X_train_rsd, X_test_rsd, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "\n",
    "        test_size=0.25,\n",
    "\n",
    "        random_state=random_state,\n",
    "\n",
    "        stratify=y  # stratify the target variable, ensures that the distribution of the target variable is the same in both training and testing sets\n",
    "\n",
    "    )\n",
    "\n",
    "    # drop rsn from X_test and X_train\n",
    "\n",
    "    X_train = X_train_rsd.drop(columns=['era5_l_sde'])\n",
    "\n",
    "    X_test = X_test_rsd.drop(columns=['era5_l_sde'])\n",
    "\n",
    "    X = X.drop(columns=['era5_l_sde'])\n",
    "\n",
    "    print('training ', X_train.shape, X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_rsd, X_test_rsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d\n",
    "# Searching for optimal hyperparameters using GridSearchCV\n",
    "# See section 2.7.2 in thesis\n",
    "# This can take a while to run, nr of hyperparameters x search k-folds x cv k-folds\n",
    "\n",
    "def run_hyperparameter_modelling(X_train, y_train, random_state):\n",
    "    \"\"\"\n",
    "    Run the modelling process using the given training data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: The input features for training.\n",
    "    - y_train: The target variable for training.\n",
    "    - random_state: The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - scores: A dictionary containing the evaluation scores for the model.\n",
    "    \"\"\"\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cv_outer = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    # define the model\n",
    "\n",
    "    model = RandomForestRegressor(random_state=random_state)\n",
    "   # define grid of hyperparameters to search\n",
    "    grid = {\n",
    "        \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=400, num=4)],\n",
    "        \"max_depth\": [int(x) for x in np.linspace(10, 50, num=5)],\n",
    "        'max_features': [0.3, 0.4, 0.5],\n",
    "    }\n",
    "    # define search. A 5-fold Grid search that iterates over the grid of hyperparameters\n",
    "    search = GridSearchCV(\n",
    "        model, grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=cv_inner\n",
    "    )\n",
    "\n",
    "    # execute the nested cross-validation\n",
    "    scores = cross_validate(\n",
    "        search,\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=('r2',\n",
    "                 'neg_mean_squared_error',\n",
    "                 'neg_mean_absolute_error',\n",
    "                 'neg_root_mean_squared_error'),\n",
    "        cv=cv_outer,\n",
    "        n_jobs=-1,\n",
    "        return_estimator=True,\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d\n",
    "# Run the modelling process using the hyperparameters from previously.\n",
    "def run_modelling(X_train, y_train, random_state):\n",
    "    \"\"\"\n",
    "    Run the modelling process using the given training data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: The input features for training.\n",
    "    - y_train: The target variable for training.\n",
    "    - random_state: The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - scores: A dictionary containing the evaluation scores for the model.\n",
    "    \"\"\"\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True,\n",
    "                     random_state=random_state)  # 10-fold cross-validation\n",
    "    # define hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 8,\n",
    "        'max_features': 0.3,\n",
    "    }\n",
    "    model = RandomForestRegressor(random_state=random_state, **params)\n",
    "    # execute the nested cross-validation\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=('r2',\n",
    "                 'neg_mean_squared_error',\n",
    "                 'neg_mean_absolute_error',\n",
    "                 'neg_root_mean_squared_error'),\n",
    "        cv=cv_outer,\n",
    "        n_jobs=-1,\n",
    "        return_estimator=True,\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing of XGboost\n",
    "# https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d\n",
    "def run_modelling(X_train, y_train, random_state):\n",
    "    # configure the cross-validation procedure\n",
    "    # cv_inner = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    # define the model\n",
    "    params = {'subsample': 0.8, 'n_estimators': 500, 'max_depth': 6,\n",
    "              'learning_rate': 0.01, 'colsample_bytree': 0.5, 'colsample_bylevel': 0.5}\n",
    "    model = XGBRegressor(random_state=random_state, **params)\n",
    "\n",
    "# new setup with just parameters\n",
    "\n",
    "    # execute the nested cross-validation\n",
    "    scores = cross_validate(\n",
    "        # search,\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=('r2',\n",
    "                 'neg_mean_squared_error',\n",
    "                 'neg_mean_absolute_error',\n",
    "                 'neg_root_mean_squared_error'),\n",
    "        cv=cv_outer,\n",
    "        n_jobs=-1,\n",
    "        return_estimator=True,\n",
    "    )\n",
    "    print(\"XGB\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A logging function to save the results of the modelling process\n",
    "def logging_to_dict(\n",
    "    scores, X_train, X_test, y_train, y_test, X_test_rsd, comment, random_state\n",
    "):\n",
    "    from numpy import std\n",
    "    from numpy import mean\n",
    "\n",
    "    def mean_error(y_test, y_pred):\n",
    "        return (y_pred - y_test).sum() / len(y_pred)\n",
    "\n",
    "    # model evaluation\n",
    "    mae_mean = mean(-scores[\"test_neg_mean_absolute_error\"])\n",
    "    mae_std = std(scores[\"test_neg_mean_absolute_error\"])\n",
    "    mse_mean = mean(-scores[\"test_neg_mean_squared_error\"])\n",
    "    mse_std = std(scores[\"test_neg_mean_squared_error\"])\n",
    "    rmse_mean = mean(-scores[\"test_neg_root_mean_squared_error\"])\n",
    "    rmse_std = std(scores[\"test_neg_root_mean_squared_error\"])\n",
    "    r2_mean = mean(scores[\"test_r2\"])\n",
    "    r2_std = std(scores[\"test_r2\"])\n",
    "\n",
    "    # best model setup\n",
    "    bestRMSE = np.argmax(scores[\"test_neg_mean_squared_error\"])\n",
    "    rfr_fit = scores[\"estimator\"]\n",
    "\n",
    "    try:\n",
    "\n",
    "        best_rfr = rfr_fit[bestRMSE].best_estimator_\n",
    "    except:\n",
    "        best_rfr = rfr_fit[bestRMSE]\n",
    "        best_rfr.fit(X_train, y_train)\n",
    "        y_pred = best_rfr.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        best_best_params = rfr_fit[bestRMSE].best_params_\n",
    "    except:\n",
    "        best_best_params = \"none\"\n",
    "\n",
    "    # test data evaluatin/best model\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = best_rfr.score(X_test, y_test)\n",
    "    me = mean_error(y_test, y_pred)\n",
    "\n",
    "    # feature imporance\n",
    "    columns = list(X_train.columns)\n",
    "    feature_imp = list(best_rfr.feature_importances_)\n",
    "\n",
    "    # convert from np to list\n",
    "    listi_y_pred = list(y_pred)\n",
    "    listi_y_test = list(y_test)\n",
    "    listi_era5_rsd = list(X_test_rsd[\"era5_l_sde\"])\n",
    "    results = {\n",
    "        \"mae_mean\": mae_mean,\n",
    "        # \"mae_std\": mae_std,\n",
    "        \"mse_mean\": mse_mean,\n",
    "        # \"mse_std\": mse_std,\n",
    "        \"rmse_mean\": rmse_mean,\n",
    "        # \"rmse_std\": rmse_std,\n",
    "        \"r2_mean\": r2_mean,\n",
    "        # \"r2_std\": r2_std,\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"me\": me,\n",
    "        \"columns\": columns,\n",
    "        \"feature_imp\": feature_imp,\n",
    "        \"best_params\": best_best_params,\n",
    "        \"comment\": comment,\n",
    "        \"y_prediction\": listi_y_pred,\n",
    "        \"y_test\": listi_y_test,\n",
    "        \"rsd_at_y_pred\": listi_era5_rsd,\n",
    "        \"random_state\": random_state,\n",
    "        \"best_rfr\": best_rfr,\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the datasets and model groups. Run the modelling process and save the results to a dictionary\n",
    "list_datasets = [\n",
    "    df_100,\n",
    "    df_500,\n",
    "    df_1000\n",
    "]\n",
    "list_of_features = [\n",
    "    'SAR',\n",
    "    'terrain',\n",
    "    'era5_land'\n",
    "]\n",
    "random_state = 210\n",
    "final_results_dict = {}\n",
    "for dataset in list_datasets:\n",
    "\n",
    "    for model_group in list_of_features:\n",
    "\n",
    "        comment = dataset['resolution'][0] + '' + model_group\n",
    "        print(comment)\n",
    "        X, y = dataset_prep(dataset, model_group)  # prepare the dataset\n",
    "        X_train, X_test, y_train, y_test, X_train_rsd, X_test_rsd = split_data(\n",
    "            X, y, random_state)  # split the data\n",
    "        # run the modelling process\n",
    "        scores = run_modelling(X_train, y_train, random_state)\n",
    "        final_results_dict[comment] = logging_to_dict(\n",
    "            scores, X_train, X_test, y_train, y_test, X_test_rsd, comment, random_state)  # save the results to a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run multiple times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same are the previous cell, but as a function that makes it possible to run the algorithm with different random states,\n",
    "# which splits the data differently and gives different results\n",
    "def run_algorithm(random_state):\n",
    "    list_datasets = [\n",
    "        df_100,\n",
    "        df_500,\n",
    "        df_1000\n",
    "    ]\n",
    "    list_of_features = [\n",
    "        'SAR',\n",
    "        'terrain',\n",
    "        'era5_land'\n",
    "    ]\n",
    "\n",
    "    final_results_dict = {}\n",
    "    for dataset in list_datasets:\n",
    "        # print(dataset)\n",
    "        for model_group in list_of_features:\n",
    "            comment = dataset['resolution'][0] + '' + model_group\n",
    "            print(comment)\n",
    "            X, y = dataset_prep(dataset, model_group)\n",
    "            X_train, X_test, y_train, y_test, X_train_rsd, X_test_rsd = split_data(\n",
    "                X, y, random_state)\n",
    "            scores = run_modelling(X_train, y_train, random_state)\n",
    "            final_results_dict[comment] = logging_to_dict(\n",
    "                scores, X_train, X_test, y_train, y_test, X_test_rsd, comment, random_state)\n",
    "    return final_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the algorithm with different random states, and save the results to a dictionary\n",
    "multiple_results = {}\n",
    "for i in range(10):\n",
    "    random_state = 42*i\n",
    "    print(random_state)\n",
    "    final_results_dict = run_algorithm(random_state)\n",
    "    print(final_results_dict)\n",
    "    multiple_results[i] = final_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileExistsError\n"
     ]
    }
   ],
   "source": [
    "name_model = \"modelling_results_10_times_constant_hyperparams_Terrain_1000m_alsoERA_depth8_LESSAR.pkl\"\n",
    "pickleName = r\"Scripts\\Python\\data_analysis\\results_fermented_pickle\"\n",
    "path_to_pickle = os.path.join(pickleName, name_model)\n",
    "try:\n",
    "    # Store the results dictionary in a pickle file\n",
    "    with open(path_to_pickle, 'xb') as handle:\n",
    "        # pickle.dump(final_results_dict, handle)\n",
    "        pickle.dump(multiple_results, handle)\n",
    "\n",
    "    print('New pickle created')\n",
    "except FileExistsError:\n",
    "    print('FileExistsError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_pickle, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datasets = [df_100, df_500, df_1000]\n",
    "list_of_features = [\n",
    "    'SAR',\n",
    "    'terrain',\n",
    "    'era5_land'\n",
    "]\n",
    "X, y = dataset_prep(list_datasets[0], list_of_features[2])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_train_rsd, X_test_rsd = split_data(\n",
    "    X, y, 42)\n",
    "X = X.drop(columns=['era5_l_sde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LearningCurveDisplay, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Define the model\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'max_features': 0.3\n",
    "}\n",
    "model = RandomForestRegressor(random_state=42, **params)\n",
    "\n",
    "# Parameters for learning curve\n",
    "common_params = {\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"train_sizes\": np.linspace(0.01, 1.0, 10),\n",
    "    \"cv\": KFold(n_splits=10, shuffle=True, random_state=42),\n",
    "    \"score_type\": \"both\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"line_kw\": {\"marker\": \"o\"},\n",
    "    \"scoring\": \"neg_root_mean_squared_error\",\n",
    "    # \"scoring\": \"r2\",\n",
    "    \"negate_score\": True,\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Generate the learning curve\n",
    "LearningCurveDisplay.from_estimator(model, **common_params, ax=ax)\n",
    "\n",
    "# Customize the legend and title\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax.set_title(f\"Learning Curve for RFR Model (ERA5 Land & SAR 1000m)\")\n",
    "ax.set_xlabel(\"Number of Training Samples\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LearningCurveDisplay, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Define the model\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 8,\n",
    "    'max_features': 0.3\n",
    "}\n",
    "model = RandomForestRegressor(random_state=42, **params)\n",
    "\n",
    "# Parameters for learning curve\n",
    "common_params = {\n",
    "    \"X\": X_train,\n",
    "    \"y\": y_train,\n",
    "    \"train_sizes\": np.linspace(0.01, 1, 10),\n",
    "    \"cv\": KFold(n_splits=10, shuffle=True, random_state=42),\n",
    "    \"score_type\": \"both\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"line_kw\": {\"marker\": \"o\"},\n",
    "    \"scoring\": \"neg_root_mean_squared_error\",\n",
    "    # \"scoring\": \"r2\",\n",
    "    \"negate_score\": True,\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Generate the learning curve\n",
    "LearningCurveDisplay.from_estimator(model, **common_params, ax=ax)\n",
    "\n",
    "# Customize the legend and title\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[:2], [\"Training Score\", \"Test Score\"])\n",
    "ax.set_title(f\"Learning Curve for RFR Model (ERA5 Land & SAR 1000m)\")\n",
    "ax.set_xlabel(\"Number of Training Samples\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make true prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe from file\n",
    "# path_to_new_prediction = r\"C:\\Users\\trygg\\Documents\\Master_project\\Data\\Prediction_SD\\data\\values_from_points.xls\"\n",
    "# path_to_new_prediction = r\"C:\\Users\\trygg\\Documents\\Master_project\\Data\\Prediction_SD\\data\\values_from_points_2018.xls\"\n",
    "path_to_new_prediction = r\"C:\\Users\\trygg\\Documents\\Master_project\\Data\\Prediction_SD\\data\\values_from_points_20190315.xls\"\n",
    "df_prediction_original = pd.read_excel(path_to_new_prediction)\n",
    "df_prediction = pd.read_excel(path_to_new_prediction)\n",
    "df_prediction.columns\n",
    "\n",
    "# add column orbitProp_ASCENDING and all = 1\n",
    "df_prediction['orbitProp_ASCENDING'] = 1\n",
    "\n",
    "df_prediction[\"time_difference_rnd\"] = 1\n",
    "df_prediction.drop(columns=[\n",
    "    # 'FID',\n",
    "    'pointid'\n",
    "], inplace=True)\n",
    "# ratio of VH/VV\n",
    "df_prediction[\"VH/VV\"] = df_prediction[\"VH\"] / df_prediction[\"VV\"]\n",
    "df_prediction.drop(columns=['sde'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_prediction.columns\n",
    "# Change columns names from df_prediction to X_train clumn names\n",
    "df_prediction.rename(columns={\"VH\": \"VH\",\n",
    "                              \"VV\": \"VV\",\n",
    "                              \"VH/VV\": \"VH/VV\",\n",
    "                              \"inc\": \"angle\",\n",
    "                              \"time_difference_rnd\": \"time_difference_rnd\",\n",
    "                              'orbitProp_ASCENDING': 'orbitProp_ASCENDING',\n",
    "                              # 'era5_l_sde': 'era5_l_sde',\n",
    "                              'TRI': 'TRI_1000',\n",
    "                              'elev': 'ELEV_1000',\n",
    "                              'asp': 'ASP_1000',\n",
    "                              'slo': 'SLO_1000',\n",
    "                              'TPI': 'TPI_1000',\n",
    "                              'skt': 'era5_l_skt',\n",
    "                              'rsn': 'era5_l_rsn',\n",
    "                              'sd': 'era5_l_sd'\n",
    "                              }, inplace=True)\n",
    "\n",
    "# arrange the order to match X_train\n",
    "df_prediction = df_prediction[[\n",
    "    'VH', 'VV', 'VH/VV', 'angle', 'time_difference_rnd', 'orbitProp_ASCENDING',\n",
    "    #    'TRI_1000', 'ELEV_1000', 'ASP_1000', 'SLO_1000', 'TPI_1000',\n",
    "    #    'era5_l_skt', 'era5_l_rsn', 'era5_l_sd'\n",
    "]]\n",
    "df_prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = final_results_dict['1000SAR']['best_rfr']\n",
    "prediction = predictor.predict(df_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = final_results_dict['1000era5_land']['best_rfr']\n",
    "prediction = predictor.predict(df_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction_original\n",
    "\n",
    "# only keep the columns we need\n",
    "df_prediction_original = df_prediction_original[[\n",
    "    # 'FID',\n",
    "    'pointid']]\n",
    "# append prediction to df_prediction_original\n",
    "df_prediction_original['SD_pred'] = prediction\n",
    "df_prediction_original\n",
    "# export to csv\n",
    "\n",
    "df_prediction_original.to_csv(\n",
    "    r\"C:\\Users\\trygg\\Documents\\Master_project\\Data\\Prediction_SD\\data\\prediction_1000_SAR_20190315.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
